{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection with Multi-feature DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this project is to use deep neural network (DNN) to detect the fraudulent credit card transactions in the multi-feature dataset. This is a classification problem, and the results by DNN are expected to show a multi-dimensional \"decision boundary\" that are hard to be defined by conventional classification techniques, e.g., SVM.\n",
    "\n",
    "# Data\n",
    "The dataset is based on kaggle's [credit card fraud detection](https://www.kaggle.com/mlg-ulb/creditcardfraud#creditcard.csv). According to the website, the dataset contains transactions occurred in two days made by credit cards in September 2013 by european cardholders. The positive class/frauds, 492 frauds out of 284,807 transactions, account for 0.172% of all transactions.\n",
    "\n",
    "The credit card data features V1, V2, ..., V28 provided are numerical principle components (PC) obtained with principle component analysis (PCA) transformation. Therefore, each feature data is already normalized by the PC variance, and has zero mean and unit variance. The original features are not allowed due to confidentiality issues. The only features without PCA transformation are 'Time' (seconds elapsed between each transaction and the first transaction in the dataset) and 'Amount' (transaction Amount). Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.  \n",
    "\n",
    "Since the PCA data are normalized, they can be worked directly as inputs for DNN. The 'Class' of 0 and 1 can be treated directly as the output using a sigmoid at the output layer.\n",
    "\n",
    "Batch data generator for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator:\n",
    "    def __init__(self,data,batchsize,ndim,nsteps,skip_steps):\n",
    "        self.data=data\n",
    "        self.batchsize=batchsize\n",
    "        self.current_idx=0\n",
    "        self.nsteps=nsteps\n",
    "        self.skip_steps = skip_steps\n",
    "        self.ndim = ndim\n",
    "    def generate(self,model):\n",
    "        x = np.zeros((self.batchsize, self.ndim))            \n",
    "        y = np.zeros((self.batchsize, 1))\n",
    "        while True:\n",
    "            for i in range(self.batchsize):\n",
    "                x, y = self.data[self.current_idx:self.current_idx+self.nsteps,:-1], self.data[self.current_idx:self.current_idx+self.nsteps,-1]\n",
    "                self.current_idx += self.skip_steps             \n",
    "                yield x, y # yield returns a set of x,y values for model.fit_generator to perform batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, valid, and test and create batches using KerasBatchGenerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import shuffle\n",
    "batchsize = 30\n",
    "nsteps = 5000 # number of samples in each batch\n",
    "ndim = 29  # feature dimensions\n",
    "hdim = 100 # hidden layer dimensions\n",
    "hlayers= 20 # hidden layers\n",
    "nepochs=50\n",
    "dropoutRate = 0.2\n",
    "datapath = 'data/'\n",
    "data = pd.read_csv('data/creditcard.csv')\n",
    "n = data.shape[0] # total sample size\n",
    "\n",
    "# scale the \"amount\" column\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "scaler.fit(data.values[:,-2].reshape(-1,1))\n",
    "amount_scaled = scaler.fit_transform(X=data.values[:,-2].reshape(-1,1))\n",
    "data = np.concatenate([data.values[:,1:-2], amount_scaled, data.values[:,-1].reshape(-1,1)],axis=1) # drop the 1st time column and replace the scaled amount\n",
    "\n",
    "data_ones  = data[[i==1 for i in data[:,-1]],:] # get ones \n",
    "data_zeros = data[[i==0 for i in data[:,-1]],:] # get zeros\n",
    "#print(len(zeros[:round(0.8*len(zeros))]))\n",
    "\n",
    "# shuffle data with ones into data with zeros\n",
    "nrepeat=300\n",
    "nz = len(data_ones)\n",
    "train = np.concatenate([data[:round(0.8*n)],             np.repeat(data_ones[:round(0.8*nz)],nrepeat,axis=0)],axis=0)\n",
    "valid = np.concatenate([data[round(0.8*n):round(0.9*n)], np.repeat(data_ones[round(0.8*nz):round(0.9*nz)],nrepeat,axis=0)],axis=0)\n",
    "#test  = np.concatenate([data[round(0.9*n):],             np.repeat(data_ones[round(0.9*nz):],nrepeat,axis=0)],axis=0)\n",
    "test  = data[round(0.9*n):]\n",
    "\n",
    "\n",
    "np.random.shuffle(train), np.random.shuffle(valid)#, np.random.shuffle(test)\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train, batchsize, ndim, nsteps, skip_steps=nsteps)\n",
    "valid_data_generator = KerasBatchGenerator(valid, batchsize, ndim, nsteps, skip_steps=nsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in above codes, the anomalies duplicated (using `np.repeat`) since they only account for 0.17% of the total data which makes the model training very hard. The data is split into 80,10,10 percent for training, validation and testing. The anomaly duplication is only done in training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model according to the local paper\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, TimeDistributed,Activation\n",
    "\n",
    "def build_model(ndim,hdim,hlayers,dropoutRate, datapath):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(ndim, input_dim= ndim, kernel_initializer='glorot_normal', activation='relu'))\n",
    "    for _ in range(hlayers-1):\n",
    "        model.add(Dropout(dropoutRate))\n",
    "        model.add(Dense(hdim, kernel_initializer='glorot_normal', activation='relu')) \n",
    "    model.add(Dense(1,kernel_initializer='glorot_normal', activation='sigmoid')) # determine the probability of anomaly \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(filepath=datapath + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "    model.summary()\n",
    "    return model, checkpointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_841 (Dense)            (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dropout_761 (Dropout)        (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "dense_842 (Dense)            (None, 100)               3000      \n",
      "_________________________________________________________________\n",
      "dropout_762 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_843 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_763 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_844 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_764 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_845 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_765 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_846 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_766 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_847 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_767 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_848 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_768 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_849 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_769 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_850 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_770 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_851 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_771 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_852 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_772 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_853 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_773 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_854 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_774 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_855 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_775 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_856 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_776 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_857 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_777 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_858 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_778 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_859 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_779 (Dropout)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_860 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_861 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 185,771\n",
      "Trainable params: 185,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 37s 18s/step - loss: 0.6911 - acc: 0.6433 - val_loss: 0.6896 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00001: saving model to data//model-01.hdf5\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 0.6715 - acc: 0.8124 - val_loss: 0.6691 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00002: saving model to data//model-02.hdf5\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 0.6182 - acc: 0.5792 - val_loss: 0.5987 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00003: saving model to data//model-03.hdf5\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 265ms/step - loss: 0.5987 - acc: 0.4731 - val_loss: 0.5982 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00004: saving model to data//model-04.hdf5\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 275ms/step - loss: 0.5571 - acc: 0.7598 - val_loss: 0.6133 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00005: saving model to data//model-05.hdf5\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 0.5385 - acc: 0.8907 - val_loss: 0.5820 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00006: saving model to data//model-06.hdf5\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.5121 - acc: 0.8959 - val_loss: 0.5085 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00007: saving model to data//model-07.hdf5\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 346ms/step - loss: 0.4816 - acc: 0.8906 - val_loss: 0.4608 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00008: saving model to data//model-08.hdf5\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 334ms/step - loss: 0.4597 - acc: 0.9007 - val_loss: 0.4446 - val_acc: 0.9415\n",
      "\n",
      "Epoch 00009: saving model to data//model-09.hdf5\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 287ms/step - loss: 0.4313 - acc: 0.9287 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00010: saving model to data//model-10.hdf5\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.4050 - acc: 0.9359 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00011: saving model to data//model-11.hdf5\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 322ms/step - loss: 0.3697 - acc: 0.9424 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00012: saving model to data//model-12.hdf5\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 263ms/step - loss: 0.3342 - acc: 0.9392 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00013: saving model to data//model-13.hdf5\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.2866 - acc: 0.9416 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00014: saving model to data//model-14.hdf5\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 318ms/step - loss: 0.2541 - acc: 0.9386 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00015: saving model to data//model-15.hdf5\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 0.2177 - acc: 0.9388 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00016: saving model to data//model-16.hdf5\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.1912 - acc: 0.9450 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00017: saving model to data//model-17.hdf5\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 0.1991 - acc: 0.9478 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00018: saving model to data//model-18.hdf5\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.2228 - acc: 0.9432 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00019: saving model to data//model-19.hdf5\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 318ms/step - loss: 0.2087 - acc: 0.9429 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00020: saving model to data//model-20.hdf5\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.1891 - acc: 0.9454 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00021: saving model to data//model-21.hdf5\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.1820 - acc: 0.9460 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00022: saving model to data//model-22.hdf5\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 290ms/step - loss: 0.1778 - acc: 0.9471 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00023: saving model to data//model-23.hdf5\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.1725 - acc: 0.9470 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00024: saving model to data//model-24.hdf5\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 0.1685 - acc: 0.9500 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00025: saving model to data//model-25.hdf5\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 286ms/step - loss: 0.1619 - acc: 0.9525 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00026: saving model to data//model-26.hdf5\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.1562 - acc: 0.9539 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00027: saving model to data//model-27.hdf5\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.1599 - acc: 0.9499 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00028: saving model to data//model-28.hdf5\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 0.1589 - acc: 0.9496 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00029: saving model to data//model-29.hdf5\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 291ms/step - loss: 0.1573 - acc: 0.9511 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00030: saving model to data//model-30.hdf5\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.1478 - acc: 0.9544 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00031: saving model to data//model-31.hdf5\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 0.1424 - acc: 0.9529 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00032: saving model to data//model-32.hdf5\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 0.1406 - acc: 0.9534 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00033: saving model to data//model-33.hdf5\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.1426 - acc: 0.9558 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00034: saving model to data//model-34.hdf5\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.1337 - acc: 0.9581 - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00035: saving model to data//model-35.hdf5\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 22ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00036: saving model to data//model-36.hdf5\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 29ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00037: saving model to data//model-37.hdf5\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 22ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00038: saving model to data//model-38.hdf5\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 24ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00039: saving model to data//model-39.hdf5\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 24ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00040: saving model to data//model-40.hdf5\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 24ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00041: saving model to data//model-41.hdf5\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 24ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00042: saving model to data//model-42.hdf5\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 23ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00043: saving model to data//model-43.hdf5\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 21ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00044: saving model to data//model-44.hdf5\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 23ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00045: saving model to data//model-45.hdf5\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 23ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00046: saving model to data//model-46.hdf5\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 21ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00047: saving model to data//model-47.hdf5\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 22ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00048: saving model to data//model-48.hdf5\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 23ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00049: saving model to data//model-49.hdf5\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 21ms/step - loss: nan - acc: nan - val_loss: nan - val_acc: nan\n",
      "\n",
      "Epoch 00050: saving model to data//model-50.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d664dd8>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "# In order to reset_states for stateful_=True in the generater, \n",
    "# we need to pass the tensorflow session to the generator\n",
    "last_epoch = 0 # set this to 0 if model never trained\n",
    "\n",
    "model, checkpointer  = build_model(ndim, hdim, hlayers, dropoutRate, datapath)\n",
    "model.fit_generator(train_data_generator.generate(model), len(train)//(batchsize*nsteps), epochs=last_epoch+nepochs,\n",
    "                validation_data=valid_data_generator.generate(model), validation_steps=math.ceil(len(valid)/(batchsize*nsteps)),\n",
    "                callbacks=[checkpointer],shuffle=True,initial_epoch=last_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the training, the accuracy approaches 1 and then becomes NAN as a result of exploding gradient when the change in weight is close to zero at the denominator.\n",
    "\n",
    "# Single batch prediction\n",
    "Use the best model to make the anomaly prediction for a number of prediction_samp samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(i_oldmodel,prediction_samp,data):\n",
    "    model = keras.models.load_model('data/model-%s.hdf5'%i_oldmodel)\n",
    "    y=[]\n",
    "    for i in range(prediction_samp):\n",
    "        curr_frame = data[i,:-1]   \n",
    "        y.append(model.predict(curr_frame[np.newaxis,:]))                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_samp=10000\n",
    "y = np.asarray(predict(35,prediction_samp,test)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of actual anomalies = 10\n",
      "total number of actual anomalies predicted = 9.0\n",
      "total number of anomalies predicted = 14\n",
      "False positive rate = 0.35714285714285715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dea51438>]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEVNJREFUeJzt3X/sXXV9x/Hni5YiVSc/2hnWQluzatZsc8A3WMLimomzsAX+2K92W/wxJnEbi5tmS4kLcywxURezGXHSTbdFEURmtGE13ab0HyuVL1MRitUKCmUqXx1qIrgWfe+Pe0pvv7a99/vt/XJ7Pzwfyc0953M+55zP53xOX73fc+73e1JVSJLacsq4GyBJGj3DXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgxePa8bJly2r16tXj2r0kTaS77777W1W1fFC9sYX76tWrmZ6eHtfuJWkiJfnaMPW8LCNJDTLcJalBhrskNchwl6QGGe6S1KCB4Z7kfUkeTXLvMZYnyTuT7EtyT5ILRt/MIdx0E6xeDaec0nu/6aaxNGNBnax9PFnbNdt82jnfvo3ymAza1om0cdkySA6/li07+vrH28dCjP+knFMns6o67gt4KXABcO8xll8OfBwIsB7YPWibVcWFF15YI/OBD1QtXVoFh19Ll/bKW9H1cRfr6y1sqV2sPzn6OCnHfj7Hb77HfJRjNWhbQ+5r166qt7yl9/7UeqeeWrtYX6/j3fU63t1bF6qWLDly/eON8UKcl5NyTo0JMF1DZOzACr1tsfo44X4jsLlvfi9wzqBtjjTcV6068kQ49Fq1anT7GLdVq2oX6+t0vl+LOFin8/3eP6Rx93FSjv18jt98j/kox2rQtobY165dVaefXrVoUe99167D6y3hiYIfFfyoTuOJwwHf39bjjfFCnJeTck6NybDhPopr7iuAh/vm93dlPybJ1Ummk0zPzMyMYNedhx6aW/kkeughdrKBAyzhhyzmAKeykw3j7+OkHPv5HL/5HvNRjtWgbQ2xr5074cAB+OEPe+87dx5e7yCn0vuhO4fX7bbb34ZjtW1BzstJOadOck/rDdWq2lpVU1U1tXz5wN+eHd55582tfBKddx4b2MkSDrCIgyzhIBvYOf4+Tsqxn8/xm+8xH+VYDdrWEPvasAGWLIFFi3rvGzYcXu9UDgIF1OF1u+32t+FYbVuQ83JSzqmT3TAf7znZL8s8E67Rec39xHjN3WvujeBpvOb+qxx5Q/Uzw2xzpOFe1Rv4Vauqkt57iyfCydrHk7Vds82nnfPt2yiPyaBtnUgbzz77yBA9++yjr3+8fSzE+E/KOTUGw4Z7enWPLcnNwAZgGfBN4K+AU7tP/e9JEuBdwEbgceA1VTXwL4JNTU2VfzhMkuYmyd1VNTWo3sC/CllVmwcsL+CP59A2SdIC8zdUJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoOGCvckG5PsTbIvyZajLD8vyR1JPpvkniSXj76pkqRhDQz3JIuAG4DLgHXA5iTrZlX7S+DWqjof2AS8e9QNlSQNb5hP7hcB+6rqgao6ANwCXDmrTgE/0U0/D/if0TVRkjRXi4eoswJ4uG9+P/CSWXXeDPxHkj8Bng1cOpLWSZLmZVQ3VDcD/1JVK4HLgfcn+bFtJ7k6yXSS6ZmZmRHtWpI02zDh/ghwbt/8yq6s31XArQBV9WngWcCy2Ruqqq1VNVVVU8uXL59fiyVJAw0T7ncBa5OsSbKE3g3TbbPqPAS8DCDJz9ALdz+aS9KYDAz3qnoSuAbYAdxP71sx9yW5PskVXbU3Aq9N8nngZuDVVVUL1WhJ0vENc0OVqtoObJ9Vdl3f9B7gktE2TZI0X/6GqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoq3JNsTLI3yb4kW45R57eS7ElyX5IPjraZkqS5WDyoQpJFwA3Ay4H9wF1JtlXVnr46a4FrgUuq6rEkP7lQDZYkDTbMJ/eLgH1V9UBVHQBuAa6cVee1wA1V9RhAVT062mZKkuZimHBfATzcN7+/K+v3QuCFST6V5M4kG4+2oSRXJ5lOMj0zMzO/FkuSBhrVDdXFwFpgA7AZ+MckZ8yuVFVbq2qqqqaWL18+ol1LkmYbJtwfAc7tm1/ZlfXbD2yrqoNV9SDwJXphL0kag2HC/S5gbZI1SZYAm4Bts+p8lN6ndpIso3eZ5oERtlOSNAcDw72qngSuAXYA9wO3VtV9Sa5PckVXbQfw7SR7gDuAP6+qby9UoyVJx5eqGsuOp6amanp6eiz7lqRJleTuqpoaVM/fUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGirck2xMsjfJviRbjlPv15NUkqnRNVGSNFcDwz3JIuAG4DJgHbA5ybqj1Hsu8Hpg96gbKUmam2E+uV8E7KuqB6rqAHALcOVR6v0N8FbgByNsnyRpHoYJ9xXAw33z+7uypyS5ADi3qv59hG2TJM3TCd9QTXIK8A7gjUPUvTrJdJLpmZmZE921JOkYhgn3R4Bz++ZXdmWHPBf4WWBnkq8C64FtR7upWlVbq2qqqqaWL18+/1ZLko5rmHC/C1ibZE2SJcAmYNuhhVX13apaVlWrq2o1cCdwRVVNL0iLJUkDDQz3qnoSuAbYAdwP3FpV9yW5PskVC91ASdLcLR6mUlVtB7bPKrvuGHU3nHizJEknwt9QlaQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQUOFe5KNSfYm2Zdky1GWvyHJniT3JPlEklWjb6okaVgDwz3JIuAG4DJgHbA5ybpZ1T4LTFXVzwO3AW8bdUMlScMb5pP7RcC+qnqgqg4AtwBX9leoqjuq6vFu9k5g5WibKUmai2HCfQXwcN/8/q7sWK4CPn60BUmuTjKdZHpmZmb4VkqS5mSkN1ST/B4wBbz9aMuramtVTVXV1PLly0e5a0lSn8VD1HkEOLdvfmVXdoQklwJvAn6pqv5vNM2TJM3HMJ/c7wLWJlmTZAmwCdjWXyHJ+cCNwBVV9ejomylJmouB4V5VTwLXADuA+4Fbq+q+JNcnuaKr9nbgOcCHk3wuybZjbE6S9DQY5rIMVbUd2D6r7Lq+6UtH3C5J0gnwN1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg4YK9yQbk+xNsi/JlqMsPy3Jh7rlu5OsHnVDJUnDWzyoQpJFwA3Ay4H9wF1JtlXVnr5qVwGPVdVPJ9kEvBX47ZG3dulSeOKJkW9WksbijDPgsccWZNMDwx24CNhXVQ8AJLkFuBLoD/crgTd307cB70qSqqqRtbQL9nV8nvtZh1eUJE2yi7iT3d+5BM48c0ECfphwXwE83De/H3jJsepU1ZNJvgucDXxrFI0E+oL950a2SUkal89wMS/hU72AXwDDhPvIJLkauBrgvPPOm/P6e1l3aEsjbJUkjUPx31y4YFsfJtwfAc7tm1/ZlR2tzv4ki4HnAd+evaGq2gpsBZiamprzJZsXsaf75D66qz2SNC4XcPeCbXuYcL8LWJtkDb0Q3wT8zqw624BXAZ8GfgP45EivtwOcfjp7nnix19wlNeEi7mQ3l/Ruqi6AgeHeXUO/BtgBLALeV1X3JbkemK6qbcB7gfcn2Qf8L73/AEbr8cdh6VL2PPHikW9aksZizN+Woaq2A9tnlV3XN/0D4DdH27SjePzxBd+FJLXAaxuS1CDDXZIaZLhLUoMMd0lqkOEuSQ3KqL+OPvSOkxnga/NcfRmj/NMGk8E+PzPY52eGE+nzqqpaPqjS2ML9RCSZrqqpcbfj6WSfnxns8zPD09FnL8tIUoMMd0lq0KSG+9ZxN2AM7PMzg31+ZljwPk/kNXdJ0vFN6id3SdJxTFy4D3pY96RIcm6SO5LsSXJfktd35Wcl+c8kX+7ez+zKk+SdXb/vSXJB37Ze1dX/cpJXjatPw0qyKMlnk9zeza/pHqy+r3vQ+pKu/JgPXk9ybVe+N8krxtOT4SQ5I8ltSb6Y5P4kF7c+zkn+rDuv701yc5JntTbOSd6X5NEk9/aVjWxck1yY5AvdOu9MMrenFFXVxLzo/cnhrwAvAJYAnwfWjbtd8+zLOcAF3fRzgS8B64C3AVu68i3AW7vpy4GP03sM1Xpgd1d+FvBA935mN33muPs3oO9vAD4I3N7N3wps6qbfA/xhN/1HwHu66U3Ah7rpdd3Ynwas6c6JRePu13H6+6/AH3TTS4AzWh5neo/dfBA4vW98X93aOAMvBS4A7u0rG9m4Ap/p6qZb97I5tW/cB2iOB/NiYEff/LXAteNu14j69jHg5cBe4Jyu7Bxgbzd9I7C5r/7ebvlm4Ma+8iPqnWwvek/y+gTwy8Dt3Yn7LWDx7DGm9wyBi7vpxV29zB73/non24veU8kepLu/NXv8WhxnDj9T+axu3G4HXtHiOAOrZ4X7SMa1W/bFvvIj6g3zmrTLMkd7WPeKMbVlZLofQ88HdgPPr6qvd4u+ATy/mz5W3yftmPwd8BfAj7r5s4HvVNWT3Xx/+4948Dpw6MHrk9TnNcAM8M/dpah/SvJsGh7nqnoE+FvgIeDr9Mbtbtoe50NGNa4ruunZ5UObtHBvTpLnAP8G/GlVfa9/WfX+y27m60xJfg14tKoW7sGRJ5/F9H50/4eqOh/4Pr0f15/S4DifCVxJ7z+2nwKeDWwca6PGYNzjOmnhPszDuidGklPpBftNVfWRrvibSc7plp8DPNqVH6vvk3RMLgGuSPJV4BZ6l2b+HjgjvQerw5Htf6pvOfLB65PU5/3A/qra3c3fRi/sWx7nS4EHq2qmqg4CH6E39i2P8yGjGtdHuunZ5UObtHB/6mHd3Z32TfQezj1xujvf7wXur6p39C069LBxuveP9ZW/srvrvh74bvfj3w7gV5Kc2X1i+pWu7KRTVddW1cqqWk1v7D5ZVb8L3EHvwerw430+dCz6H7y+DdjUfctiDbCW3s2nk05VfQN4OMmLuqKXAXtoeJzpXY5Zn2Rpd54f6nOz49xnJOPaLftekvXdMXxl37aGM+4bEvO4gXE5vW+WfAV407jbcwL9+EV6P7LdA3yue11O71rjJ4AvA/8FnNXVD3BD1+8vAFN92/p9YF/3es24+zZk/zdw+NsyL6D3j3Yf8GHgtK78Wd38vm75C/rWf1N3LPYyx28RjKGvvwBMd2P9UXrfimh6nIG/Br4I3Au8n943XpoaZ+BmevcUDtL7Ce2qUY4rMNUdv68A72LWTflBL39DVZIaNGmXZSRJQzDcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq0P8DD+jRfjd4AVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame as df\n",
    "import time\n",
    "test_data = test[:prediction_samp,-1].reshape(-1,1)\n",
    "#plt.plot(range(0,prediction_samp),test_data,'b.')\n",
    "y1 = np.array(y)\n",
    "y1[np.asarray([i>0.5 for i in y])] = 1\n",
    "y1[np.asarray([i<=0.5 for i in y])] = 0\n",
    "#y[i<=0.5 for i in y]=0\n",
    "plt.plot(range(0,prediction_samp),y1,'ro')\n",
    "npreda = np.count_nonzero(y1)\n",
    "i_preda = [i==1 for i in y1] # predicted anomaly indices\n",
    "nactual = sum(i==1 for i in test_data)\n",
    "naccurate = npreda - abs(sum(y1[i_preda]-test_data[i_preda])) \n",
    "print('total number of actual anomalies = '+ str(*nactual))\n",
    "print('total number of actual anomalies predicted = '+str(naccurate))\n",
    "print('total number of anomalies predicted = '+str(npreda))\n",
    "print('False positive rate = '+str((npreda-naccurate)/npreda))\n",
    "plt.plot(range(0,prediction_samp),test_data,'b.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "As shown in the above figure, with a sample of 10000, the algorithm captured 9 out of 10 actual anomalies (blue dots at ones). The False positive rate is 0.36, which is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The project shows that by duplicating the anomaly in the training process, the model can be well trained with DNN to capture the anomalies using multi-dimensional features. The false positive rate is low but I hope to reduce it further by tweaking the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
